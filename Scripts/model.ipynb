{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document qui vise à explorer le dataset de la base de données de KBO\n",
    "\n",
    "#Importation des librairies\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifier les types de données\n",
    "dtypes_activity = {\n",
    "    'EntityNumber': 'object',\n",
    "    'ActivityGroup': 'int64',\n",
    "    'NaceVersion': 'int64',\n",
    "    'NaceCode': 'int64',\n",
    "    'Classification': 'object'\n",
    "}\n",
    "\n",
    "dtypes_address = {\n",
    "    'EntityNumber': 'object',\n",
    "    'TypeOfAddress': 'object',\n",
    "    'CountryNL': 'object',\n",
    "    'CountryFR': 'object',\n",
    "    'Zipcode': 'object',  # Spécifier Zipcode comme objet\n",
    "    'MunicipalityNL': 'object',\n",
    "    'MunicipalityFR': 'object',\n",
    "    'StreetNL': 'object',\n",
    "    'StreetFR': 'object',\n",
    "    'HouseNumber': 'object',\n",
    "    'Box': 'object',\n",
    "    'ExtraAddressInfo': 'object',\n",
    "    'DateStrikingOff': 'object'\n",
    "}\n",
    "\n",
    "dtypes_enterprise = {\n",
    "    'EnterpriseNumber': 'object',\n",
    "    'Status': 'object',\n",
    "    'JuridicalSituation': 'int64',\n",
    "    'TypeOfEnterprise': 'int64',\n",
    "    'JuridicalForm': 'float64',  # Peut contenir des NaNs\n",
    "    'JuridicalFormCAC': 'float64',  # Peut contenir des NaNs\n",
    "    'StartDate': 'object'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin relatif vers le dossier de données\n",
    "data_dir = os.path.join('..', '..', 'data')\n",
    "\n",
    "# Chemins complets vers chaque fichier CSV\n",
    "activity_path = os.path.join(data_dir, 'activity.csv')\n",
    "address_path = os.path.join(data_dir, 'address.csv')\n",
    "enterprise_path = os.path.join(data_dir, 'enterprise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des datasets\n",
    "activity_df = dd.read_csv(activity_path, dtype=dtypes_activity)\n",
    "address_df = dd.read_csv(address_path, dtype=dtypes_address)\n",
    "enterprise_df = dd.read_csv(enterprise_path, dtype=dtypes_enterprise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def optimize_dataframe(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif col_type == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "        elif col_type == 'object':\n",
    "            num_unique_values = len(df[col].unique())\n",
    "            num_total_values = len(df[col])\n",
    "            if num_unique_values / num_total_values < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "# Optimiser les datasets\n",
    "activity_df = optimize_dataframe(activity_df)\n",
    "address_df = optimize_dataframe(address_df)\n",
    "enterprise_df = optimize_dataframe(enterprise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner les datasets en utilisant EntityNumber comme clé primaire\n",
    "merged_df = activity_df.merge(address_df, on='EntityNumber', how='left')\n",
    "merged_df = merged_df.merge(enterprise_df, left_on='EntityNumber', right_on='EnterpriseNumber', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les caractéristiques pertinentes\n",
    "features = merged_df[['ActivityGroup', 'NaceVersion', 'NaceCode', 'Zipcode', 'JuridicalSituation', 'TypeOfEnterprise']]\n",
    "target = merged_df['Classification'].apply(lambda x: 1 if x == 'MAIN' else 0, meta=('x', 'int64'))  # Convertir MAIN à 1 et SECONDARY à 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les valeurs manquantes pour les colonnes numériques et non numériques\n",
    "def fill_missing_values(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        else:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df\n",
    "\n",
    "# Appliquer la fonction aux partitions\n",
    "features = features.map_partitions(fill_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Traiter les partitions une par une\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mto_delayed():\n\u001b[1;32m---> 11\u001b[0m     X_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcompute_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m target\u001b[38;5;241m.\u001b[39mto_delayed():\n\u001b[0;32m     14\u001b[0m     y_list\u001b[38;5;241m.\u001b[39mappend(compute_partition(part))\n",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m, in \u001b[0;36mcompute_partition\u001b[1;34m(part)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_partition\u001b[39m(part):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nawfel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\base.py:376\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Nawfel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask\\base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\Nawfel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\dask_expr\\_shuffle.py:500\u001b[0m, in \u001b[0;36mDiskShuffle._shuffle_group\u001b[1;34m(df, col, _filter, p)\u001b[0m\n\u001b[0;32m    498\u001b[0m g \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(col)\n\u001b[0;32m    499\u001b[0m d \u001b[38;5;241m=\u001b[39m {i: g\u001b[38;5;241m.\u001b[39mget_group(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m g\u001b[38;5;241m.\u001b[39mgroups \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _filter}\n\u001b[1;32m--> 500\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfsync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nawfel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\partd\\encode.py:24\u001b[0m, in \u001b[0;36mEncode.append\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     23\u001b[0m     data \u001b[38;5;241m=\u001b[39m valmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode, data)\n\u001b[1;32m---> 24\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mvalmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartd\u001b[38;5;241m.\u001b[39mappend(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nawfel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\toolz\\dicttoolz.py:85\u001b[0m, in \u001b[0;36mvalmap\u001b[1;34m(func, d, factory)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Apply function to values of dictionary\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m>>> bills = {\"Alice\": [20, 15, 30], \"Bob\": [10, 35]}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    itemmap\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m rv \u001b[38;5;241m=\u001b[39m factory()\n\u001b[1;32m---> 85\u001b[0m \u001b[43mrv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rv\n",
      "File \u001b[1;32mc:\\Users\\Nawfel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\partd\\utils.py:40\u001b[0m, in \u001b[0;36mframe\u001b[1;34m(bytes)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mframe\u001b[39m(\u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Pack the length of the bytes in front of the bytes\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    TODO: This does a full copy.  This should maybe be inlined somehow\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    wherever this gets used instead.  My laptop shows a data bandwidth of\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    2GB/s\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstruct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convertir les DataFrames Dask en DataFrames pandas par partitions\n",
    "def compute_partition(part):\n",
    "    return part.compute()\n",
    "\n",
    "# Initialiser des listes pour stocker les partitions pandas\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# Traiter les partitions une par une\n",
    "for part in features.to_delayed():\n",
    "    X_list.append(compute_partition(part))\n",
    "\n",
    "for part in target.to_delayed():\n",
    "    y_list.append(compute_partition(part))\n",
    "\n",
    "# Concaténer les partitions en un seul DataFrame pandas\n",
    "X = pd.concat(X_list, ignore_index=True)\n",
    "y = pd.concat(y_list, ignore_index=True)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraîner un modèle de classification (Random Forest)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluer le modèle\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, 'model.pkl')  # Sauvegarder le modèle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
